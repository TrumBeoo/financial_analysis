"""
Script ƒë·ªÉ crawl tin t·ª©c t·ª´ command line
"""
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import logging
from datetime import datetime
from src.crawler.news_crawler import FinancialNewsCrawler
from src.database.db_manager import DatabaseManager
from src.processing.text_preprocessor import VietnameseTextPreprocessor
from src.processing.sentiment_analyzer import SentimentAnalyzer
from src.models.classifier import NewsClassifier
import pandas as pd

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    """Ch·∫°y crawler v√† l∆∞u v√†o database"""
    
    logger.info("üöÄ B·∫Øt ƒë·∫ßu crawl tin t·ª©c...")
    
    # Kh·ªüi t·∫°o
    crawler = FinancialNewsCrawler()
    db_manager = DatabaseManager()
    preprocessor = VietnameseTextPreprocessor()
    sentiment_analyzer = SentimentAnalyzer()
    
    # Crawl
    df_news = crawler.crawl_all(max_workers=3)
    
    if df_news.empty:
        logger.warning("‚ö†Ô∏è Kh√¥ng crawl ƒë∆∞·ª£c tin t·ª©c n√†o!")
        return
    
    logger.info(f"‚úì Crawled {len(df_news)} articles")
    
    # Ki·ªÉm tra content
    if 'content' in df_news.columns:
        avg_length = df_news['content'].str.len().mean()
        logger.info(f"üìä Average content length: {avg_length:.0f} chars")
    else:
        logger.warning("‚ö†Ô∏è No content field in crawled data!")
    
    # L∆∞u d·ªØ li·ªáu g·ªëc
    db_manager.save_news_data(df_news)
    
    # X·ª≠ l√Ω t·ª´ng b√†i
    processed_data = []
    
    for idx, row in df_news.iterrows():
        try:
            # S·ª¨A: S·ª≠ d·ª•ng full content thay v√¨ ch·ªâ title + summary
            if 'content' in row and row['content']:
                full_text = f"{row['title']} {row['content']}"
            else:
                full_text = f"{row['title']} {row.get('summary', '')}"
            
            logger.info(f"Processing article {idx+1}/{len(df_news)}: {row['title'][:50]}...")
            logger.info(f"  Text length: {len(full_text)} chars")
            
            # Preprocess
            processed = preprocessor.preprocess_pipeline(full_text)
            
            # Analyze sentiment
            sentiment = sentiment_analyzer.analyze(full_text)
            
            logger.info(f"  Sentiment: {sentiment['label']} | Sectors: {processed['sectors']}")
            
            processed_data.append({
                'source': row['source'],
                'title': row['title'],
                'summary': row.get('summary', ''),
                'content': row.get('content', ''),  # TH√äM: L∆∞u full content
                'link': row['link'],
                'crawl_time': row['crawl_time'],
                'cleaned_text': processed['cleaned_text'],
                'sentiment_positive': sentiment['positive'],
                'sentiment_negative': sentiment['negative'],
                'sentiment_neutral': sentiment['neutral'],
                'predicted_label': sentiment['label'],
                'predicted_sentiment': ['Ti√™u c·ª±c', 'Trung t√≠nh', 'T√≠ch c·ª±c'][sentiment['label']],
                'sectors': ','.join(processed['sectors']),
                'processed_at': datetime.now()
            })
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói x·ª≠ l√Ω b√†i {idx}: {e}")
            continue
    
    # L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
    if processed_data:
        df_processed = pd.DataFrame(processed_data)
        db_manager.save_processed_data(df_processed)
        logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω v√† l∆∞u {len(df_processed)} b√†i vi·∫øt")
        
        # Statistics
        logger.info(f"\nüìä TH·ªêNG K√ä:")
        logger.info(f"  - T√≠ch c·ª±c: {len(df_processed[df_processed['predicted_label']==2])}")
        logger.info(f"  - Trung t√≠nh: {len(df_processed[df_processed['predicted_label']==1])}")
        logger.info(f"  - Ti√™u c·ª±c: {len(df_processed[df_processed['predicted_label']==0])}")
        
        if 'content' in df_processed.columns:
            logger.info(f"  - Avg content: {df_processed['content'].str.len().mean():.0f} chars")
    
    logger.info("‚úÖ Ho√†n th√†nh!")

if __name__ == '__main__':
    main()